algorithm_kwargs:
  batch_size: 8
  eval_freq: 1
  num_epochs: 200
  start_epoch: 16
amp: false
dataset_kwargs:
  batch_size: 8
  crop_size: 512
  dataset: bing_rgb
  image_size: 512
  normalization: vit
  num_workers: 10
  split: train
  tokenizer_path: openai/clip-vit-base-patch16
inference_kwargs:
  im_size: 512
  window_size: 512
  window_stride: 341
log_dir: seg_tiny_mask
net_kwargs:
  backbone: vit_base_patch16_384
  d_model: 768
  decoder:
    drop_path_rate: 0.0
    dropout: 0.1
    n_cls: 6
    n_layers: 2
    name: mask_transformer
  distilled: false
  dlg:
    d_model: 1024
  drop_path_rate: 0.1
  dropout: 0.0
  image_size: !!python/tuple
  - 512
  - 512
  n_cls: 6
  n_heads: 12
  n_layers: 12
  normalization: vit
  patch_size: 16
  text_encoder:
    d_model: 512
    model_path: openai/clip-vit-base-patch16
optimizer_kwargs:
  clip_grad: null
  epochs: 200
  iter_max: 2200
  iter_warmup: 0.0
  lr: 0.001
  min_lr: 1.0e-05
  momentum: 0.9
  opt: sgd
  poly_power: 0.9
  poly_step_size: 1
  sched: polynomial
  weight_decay: 0.0
resume: true
version: normal
world_batch_size: 8
